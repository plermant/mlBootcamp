{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f73a819f",
   "metadata": {},
   "source": [
    "This is two-part application.\n",
    "The first part, model creation, is this Notebook, where different models are trained and saved to file.\n",
    "The second part is the streamilt application.\n",
    "To run the app, launch \"streamlit run appName.py\", where appName .py is the code right below placed in a separate python file on the machine where the app is run. It generates an url user can paste in their browser to interract with the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61c667bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e38e85fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global variables\n",
    "percentVal=.2\n",
    "batch_size = 32\n",
    "maxTokens = 20000 # max voclbulary size\n",
    "max_length = 600 # max number of words in document\n",
    "epochCount=1\n",
    "embed_dim = 256 # for Transformer implementation\n",
    "num_heads = 2 # for Transformer implementation\n",
    "dense_dim = 32 # for Transformer implementation\n",
    "posString = \"That was an excellent movie, I loved it.\"\n",
    "negString = \"That was a horrible movie, I hated it.\"\n",
    "nlpRoot=\"C:/Users/lerma/machineLearning/NLP\"\n",
    "trainPath=nlpRoot+'/aclImdb/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcefc0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\n",
      "  0 80.2M    0  176k    0     0  51793      0  0:27:04  0:00:03  0:27:01 51803\n",
      "  7 80.2M    7 6080k    0     0  1367k      0  0:01:00  0:00:04  0:00:56 1367k\n",
      " 20 80.2M   20 16.6M    0     0  3133k      0  0:00:26  0:00:05  0:00:21 3133k\n",
      " 34 80.2M   34 27.3M    0     0  4349k      0  0:00:18  0:00:06  0:00:12 4672k\n",
      " 47 80.2M   47 38.0M    0     0  5237k      0  0:00:15  0:00:07  0:00:08 7881k\n",
      " 60 80.2M   60 48.1M    0     0  5836k      0  0:00:14  0:00:08  0:00:06 9891k\n",
      " 73 80.2M   73 58.8M    0     0  6379k      0  0:00:12  0:00:09  0:00:03 10.5M\n",
      " 86 80.2M   86 69.5M    0     0  6815k      0  0:00:12  0:00:10  0:00:02 10.5M\n",
      " 99 80.2M   99 80.2M    0     0  7177k      0  0:00:11  0:00:11 --:--:-- 10.5M\n",
      "100 80.2M  100 80.2M    0     0  7179k      0  0:00:11  0:00:11 --:--:-- 10.5M\n"
     ]
    }
   ],
   "source": [
    "#fetch files and untar, to be performed only once in project\n",
    "# !curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "# !tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57d14252",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put percentVal of samples in a validation directory, to be performed only once in project\n",
    "# base_dir = pathlib.Path(\"aclImdb\")\n",
    "# val_dir = base_dir / \"val\" \n",
    "# train_dir = base_dir / \"train\" \n",
    "# for category in (\"neg\", \"pos\"):\n",
    "#     os.makedirs(val_dir / category)\n",
    "#     files = os.listdir(train_dir / category)\n",
    "#     random.Random(1337).shuffle(files)\n",
    "#     num_val_samples = int(percentVal * len(files))\n",
    "#     val_files = files[-num_val_samples:]\n",
    "#     for fname in val_files:\n",
    "#         shutil.move(train_dir / category / fname,\n",
    "#                     val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d36f0f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Letâ€™s create three Dataset objects for training, validation, and testing:\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\", batch_size=batch_size\n",
    ")\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/val\", batch_size=batch_size\n",
    ")\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2da0f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_ds = keras.utils.text_dataset_from_directory(trainPath, batch_size=batch_size)\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "\n",
    "text_vectorizationTfIdf = TextVectorization(ngrams=2,max_tokens=maxTokens,output_mode=\"tf-idf\")\t\n",
    "text_vectorizationTfIdf.adapt(text_only_train_ds)\n",
    "\n",
    "text_vectorizationBidiLSTM = layers.TextVectorization(max_tokens=maxTokens,output_mode=\"int\",output_sequence_length=max_length,)\n",
    "text_vectorizationBidiLSTM.adapt(text_only_train_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2036ac00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32,)\n",
      "inputs.dtype: <dtype: 'string'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor(b\"I've seen this movie and I must say I'm very impressed. There are not much movies I like, but I do like this one. You should see this movie by yourself and comment it,because this is one of my most favorite movie. I fancy to see this again. Action fused with a fantastic story. Very impressing. I like Modesty's character. Actually she's very mystic and mysterious (I DO like that^^). The bad boy is pretty too. Well, actually this whole movie is rare in 'movieworld'. I considered about the vote of this movie, I thought this is should be a very popular movie. I guess wrong. It was ME who was very impressed about this movie, and I hope I'm not the only one who takes only the cost to watch this one. See and vote.\", shape=(), dtype=string)\n",
      "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Displaying the shapes and dtypes of the first batch\n",
    "for inputs, targets in train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f40ce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our model-building utility\n",
    "  \n",
    "def get_model(max_tokens=maxTokens, hidden_dim=16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f6398a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelFromName(name):\n",
    "    path=nlpRoot+\"/models/\"+name\n",
    "    if name=='bagBigramTdIdf'or name=='embedBidiLSTM':\n",
    "        return tf.keras.models.load_model(path) \n",
    "    elif name == \"transformer\":\n",
    "        return tf.keras.models.load_model(path, custom_objects={\"TransformerEncoder\": TransformerEncoder,\"PositionalEmbedding\": PositionalEmbedding})\n",
    "    else:\n",
    "        print(\"Error in model name, does not exist, aborting ...\")\n",
    "        exit(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70ab1c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSentiment(inString,modelName):\n",
    "    tfString=tf.convert_to_tensor([[inString],])\n",
    "    model=modelFromName(modelName)\n",
    "    inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
    "    if modelName=='bagBigramTdIdf':\n",
    "        processed_inputs = text_vectorizationTfIdf(inputs)\n",
    "    elif modelName=='embedBidiLSTM' or modelName == 'transformer':\n",
    "        processed_inputs = text_vectorizationBidiLSTM(inputs)\n",
    "    else:\n",
    "        print(\"Can't vectorize this model, exiting ..\")\n",
    "        exit(1) \n",
    "    outputs = model(processed_inputs)\n",
    "    inference_model = keras.Model(inputs, outputs)\n",
    "    predictions = inference_model(tfString)\n",
    "    resp=round(float(predictions[0] * 100),2)\n",
    "    return resp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d22312ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring the TextVectorization layer to return bigrams with TF-IDF\n",
    "text_vectorizationTfIdf = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=maxTokens,\n",
    "    output_mode=\"tf-idf\"\n",
    ")\n",
    "text_vectorizationTfIdf.adapt(text_only_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "18fa297a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_14 (InputLayer)       [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "625/625 [==============================] - 36s 55ms/step - loss: 0.4953 - accuracy: 0.7780 - val_loss: 0.3254 - val_accuracy: 0.8760\n",
      "782/782 [==============================] - 65s 81ms/step - loss: 0.3252 - accuracy: 0.8709\n",
      "tfidf_2gram test accuracy: 0.871\n"
     ]
    }
   ],
   "source": [
    "# Training and testing the TF-IDF bigram model\n",
    " \n",
    "tfidf_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorizationTfIdf(x), y),\n",
    "    num_parallel_calls=4)\n",
    "tfidf_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorizationTfIdf(x), y),\n",
    "    num_parallel_calls=4)\n",
    "tfidf_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorizationTfIdf(x), y),\n",
    "    num_parallel_calls=4)\n",
    " \n",
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(tfidf_2gram_train_ds.cache(),\n",
    "          validation_data=tfidf_2gram_val_ds.cache(),\n",
    "          epochs=epochCount,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
    "print(f\"tfidf_2gram test accuracy: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45410da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:/Users/lerma/machineLearning/NLP/models/bagBigramTdIdf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:/Users/lerma/machineLearning/NLP/models/bagBigramTdIdf\\assets\n"
     ]
    }
   ],
   "source": [
    "# Saving/loading model\n",
    "path=nlpRoot+'/models'+'/bagBigramTdIdf'\n",
    "model.save(path)\n",
    "model=keras.models.load_model(path) # to make sure we can retrieve it later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8223ad3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.49 percent positive for string That was an excellent movie, I loved it. with model bagBigramTdIdf\n",
      "22.81 percent positive for string That was a horrible movie, I hated it. with model bagBigramTdIdf\n"
     ]
    }
   ],
   "source": [
    "print(predictSentiment(posString,'bagBigramTdIdf'),\"percent positive for string\",posString,'with model bagBigramTdIdf')\n",
    "print(predictSentiment(negString,'bagBigramTdIdf'),\"percent positive for string\",negString,'with model bagBigramTdIdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a75096cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets do embedding with bidi LSTM - Takes a while to fit, so set number of epoch to very low number\n",
    "  \n",
    "text_vectorizationBidiLSTM = layers.TextVectorization(\n",
    "    max_tokens=maxTokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length\n",
    ")\n",
    "text_vectorizationBidiLSTM.adapt(text_only_train_ds)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4569e497",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorizationBidiLSTM(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorizationBidiLSTM(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorizationBidiLSTM(x), y),\n",
    "    num_parallel_calls=4)\n",
    "\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = layers.Embedding(\n",
    "    input_dim=maxTokens, output_dim=256, mask_zero=True)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru_with_masking.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=epochCount,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"embeddings_bidir_gru_with_masking.keras\") \n",
    "print(f\"bidiLSTM test accuracy: {model.evaluate(int_test_ds)[1]:.3f}\")\n",
    "\n",
    "#Saving/loading model\n",
    "path=nlpRoot+'/models'+'/embedBidiLSTM'\n",
    "model.save(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "177537d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.12 percent positive for string That was an excellent movie, I loved it. with model embedBidiLSTM\n",
      "14.72 percent positive for string That was a horrible movie, I hated it. with model embedBidiLSTM\n"
     ]
    }
   ],
   "source": [
    "print(predictSentiment(posString,'embedBidiLSTM'),\"percent positive for string\",posString,'with model embedBidiLSTM')\n",
    "print(predictSentiment(negString,'embedBidiLSTM'),\"percent positive for string\",negString,'with model embedBidiLSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "42bb6727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer implementation\n",
    "  \n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(\n",
    "            inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "  \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "  \n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    " \n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    " \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c96613f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_11 (InputLayer)       [(None, None)]            0         \n",
      "                                                                 \n",
      " positional_embedding (Posit  (None, None, 256)        5273600   \n",
      " ionalEmbedding)                                                 \n",
      "                                                                 \n",
      " transformer_encoder (Transf  (None, None, 256)        543776    \n",
      " ormerEncoder)                                                   \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 256)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,817,633\n",
      "Trainable params: 5,817,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "625/625 [==============================] - 3594s 6s/step - loss: 0.5326 - accuracy: 0.7412 - val_loss: 0.3295 - val_accuracy: 0.8566\n",
      "782/782 [==============================] - 2952s 4s/step - loss: 0.3141 - accuracy: 0.8631\n",
      "For Transformer, test accuracy is: 0.863\n"
     ]
    }
   ],
   "source": [
    "# Training and evaluation, Combining the Transformer encoder with positional embedding\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = PositionalEmbedding(max_length, maxTokens, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "  \n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\",\n",
    "                                    save_best_only=True)\n",
    "] \n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=epochCount, callbacks=callbacks)\n",
    "model = keras.models.load_model(\n",
    "    \"full_transformer_encoder.keras\",\n",
    "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
    "                    \"PositionalEmbedding\": PositionalEmbedding}) \n",
    "print(f\"For Transformer, test accuracy is: {model.evaluate(int_test_ds)[1]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3395e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving model\n",
    "path=nlpRoot+'/models'+'/transformer'\n",
    "model.save(path,save_format='h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c91bb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.56 percent positive for string That was an excellent movie, I loved it. with model transformer\n",
      "1.37 percent positive for string That was a horrible movie, I hated it. with model transformer\n"
     ]
    }
   ],
   "source": [
    "# print(predictSentiment(posString,'bagBigramTdIdf'),\"percent positive for string\",posString,'with model bagBigramTdIdf')\n",
    "# print(predictSentiment(negString,'bagBigramTdIdf'),\"percent positive for string\",negString,'with model bagBigramTdIdf')\n",
    "# print(predictSentiment(posString,'embedBidiLSTM'),\"percent positive for string\",posString,'with model embedBidiLSTM')\n",
    "# print(predictSentiment(negString,'embedBidiLSTM'),\"percent positive for string\",negString,'with model embedBidiLSTM')\n",
    "print(predictSentiment(posString,'transformer'),\"percent positive for string\",posString,'with model transformer')\n",
    "print(predictSentiment(negString,'transformer'),\"percent positive for string\",negString,'with model transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5c46f87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save text vectorizations to file so it canbe pre-computed and loaded from streamlit app\n",
    "import pickle \n",
    "\n",
    "fileName=nlpRoot+'/models'+'/vectorLSTM.pkl'\n",
    "pickle.dump({'config': text_vectorizationBidiLSTM.get_config(),\n",
    "             'weights': text_vectorizationBidiLSTM.get_weights()}\n",
    "            , open(fileName, \"wb\"))\n",
    "\n",
    "fileName=nlpRoot+'/models'+'/vectorTfIdf.pkl'\n",
    "pickle.dump({'config': text_vectorizationTfIdf.get_config(),\n",
    "             'weights': text_vectorizationTfIdf.get_weights()}\n",
    "            , open(fileName, \"wb\"))\n",
    "\n",
    "\n",
    "fileName=nlpRoot+'/models'+'/vectorLSTM.pkl'\n",
    "from_disk = pickle.load(open(fileName, \"rb\"))\n",
    "text_vectorizationBidiLSTM = TextVectorization.from_config(from_disk['config'])\n",
    "# You have to call `adapt` with some dummy data (BUG in Keras)\n",
    "#new_v.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n",
    "text_vectorizationBidiLSTM.set_weights(from_disk['weights'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d858a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# streamlit application, for reference, as it's a separate python file.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import streamlit as st\n",
    "import pickle\n",
    "\n",
    "st.title('Movie sentiment across models')\n",
    "\n",
    "\n",
    "#Global variables\n",
    "maxTokens = 20000 # max vocabulary size\n",
    "max_length=600 # max document size\n",
    "batch_size=32\n",
    "# to be run at \"C:/Users/plermant/git/miniprojects/NLPpractice\"\n",
    "nlpRoot=\".\"\n",
    "\n",
    "trainPath=nlpRoot+\"/aclImdb/train\"\n",
    "\n",
    "@st.cache_resource\n",
    "def getTrainDS(path, batch_size=batch_size):\n",
    "    return keras.utils.text_dataset_from_directory(path, batch_size=batch_size)\n",
    "    \n",
    "@st.cache_resource \n",
    "def getTextTrainDS(_ds):\n",
    "    return _ds.map(lambda x, y: x)\n",
    "    \n",
    "train_ds = getTrainDS(trainPath, batch_size=batch_size)\n",
    "text_only_train_ds = getTextTrainDS(train_ds)\n",
    "\n",
    "fileName=nlpRoot+'/models'+'/vectorTfIdf.pkl'\n",
    "from_disk = pickle.load(open(fileName, \"rb\"))\n",
    "text_vectorizationTfIdf = TextVectorization.from_config(from_disk['config'])\n",
    "text_vectorizationTfIdf.set_weights(from_disk['weights'])\n",
    "\n",
    "\n",
    "fileName=nlpRoot+'/models'+'/vectorLSTM.pkl'\n",
    "from_disk = pickle.load(open(fileName, \"rb\"))\n",
    "text_vectorizationBidiLSTM = TextVectorization.from_config(from_disk['config'])\n",
    "text_vectorizationBidiLSTM.set_weights(from_disk['weights'])\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(\n",
    "            inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "  \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config\n",
    "        \n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "  \n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    " \n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    " \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "@st.cache_resource\n",
    "def modelFromName(name):\n",
    "    path=nlpRoot+\"/models/\"+name\n",
    "    if name=='bagBigramTdIdf' or name =='embedBidiLSTM':\n",
    "        return tf.keras.models.load_model(path)\t\n",
    "    elif name == \"transformer\":\n",
    "        return tf.keras.models.load_model(path, custom_objects={\"TransformerEncoder\": TransformerEncoder,\"PositionalEmbedding\": PositionalEmbedding})\n",
    "    else:\n",
    "        print(\"Error in model name, does not exist, aborting ...\")\n",
    "        exit(1)\n",
    " \n",
    "@st.cache_resource \n",
    "def predictSentiment(inString,modelName):\n",
    "    tfString=tf.convert_to_tensor([[inString],])\n",
    "    model=modelFromName(modelName)\n",
    "    inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
    "    if modelName=='bagBigramTdIdf':\n",
    "        processed_inputs = text_vectorizationTfIdf(inputs)\n",
    "    elif modelName=='embedBidiLSTM' or modelName == 'transformer':\n",
    "        processed_inputs = text_vectorizationBidiLSTM(inputs)\n",
    "    outputs = model(processed_inputs)\n",
    "    inference_model = keras.Model(inputs, outputs)\n",
    "    predictions = inference_model(tfString) \n",
    "    resp=round(float(predictions[0] * 100),2)\n",
    "    return resp\n",
    "\t\n",
    "modelName= st.selectbox(\"Select a ML model from this list\",(\"bagBigramTdIdf\", 'embedBidiLSTM',\"transformer\"))\n",
    "inputString= st.text_input('What do you think of the movie? e.g. \"I loved the movie\", or \"This movie sucked\"')\n",
    "\n",
    "sentimentPercent=predictSentiment(inputString,modelName)\n",
    "st.write(\"Sentiment value is\",sentimentPercent,\"per\",modelName,\"model.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
